{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "# def read_secret(secret_name):\n",
    "#     secret_path = os.getenv(secret_name)\n",
    "#     try:\n",
    "#         with open(secret_path, 'r') as file:\n",
    "#             return file.read().strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {secret_name}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# INFLUXDB_USERNAME = read_secret('DOCKER_INFLUXDB_INIT_USERNAME_FILE')\n",
    "# INFLUXDB_PASSWORD = read_secret('DOCKER_INFLUXDB_INIT_PASSWORD_FILE')\n",
    "# INFLUXDB_TOKEN = read_secret('DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE')\n",
    "\n",
    "# INFLUXDB_ORG = os.getenv('DOCKER_INFLUXDB_INIT_ORG')\n",
    "# INFLUXDB_BUCKET = os.getenv('DOCKER_INFLUXDB_INIT_BUCKET')\n",
    "# INFLUXDB_URL = os.getenv('INFLUXDB_URL', 'http://influxdb:8086') \n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.query_api import QueryOptions\n",
    "import pandas as pd\n",
    "import os\n",
    "INFLUXDB_USERNAME=\"admin\"\n",
    "INFLUXDB_PASSWORD=\"password\"\n",
    "INFLUXDB_TOKEN=\"G3UtSut5Kv-RuT32yh27StdDCrl4fu3uzxPLzdias8vFsNzyzgfw5kIX9iGvtLctAXpZjFItOUwA65YWtk_5fg==\"\n",
    "INFLUXDB_ORG=\"example_org\"\n",
    "INFLUXDB_BUCKET=\"example_bucket\"\n",
    "INFLUXDB_URL=\"http://localhost:8086\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        noisy             original_time  threshold_anomaly  anomaly\n",
      "3   71.067422 2018-07-23 09:00:00+00:00               True     True\n",
      "4   70.323179 2018-07-23 12:00:00+00:00               True     True\n",
      "5   70.554334 2018-07-23 15:00:00+00:00               True     True\n",
      "6   71.103025 2018-07-23 18:00:00+00:00               True     True\n",
      "15  71.463617 2018-07-26 09:00:00+00:00               True     True\n",
      "16  70.765219 2018-07-26 12:00:00+00:00               True     True\n",
      "17  70.815922 2018-07-26 15:00:00+00:00               True     True\n",
      "18  70.286680 2018-07-26 18:00:00+00:00               True     True\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "\n",
    "class AnomalyDetector():\n",
    "    def __init__(self,client):\n",
    "        self.client = client\n",
    "        self.query_api = client.query_api(query_options=QueryOptions(profilers=[]))\n",
    "        self.training_df = pd.DataFrame()\n",
    "        self.upcoming_df = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.get_dataframe_initial()\n",
    "        #self.train_model()\n",
    "        while True:\n",
    "            new_df = self.get_dataframe_starting(self.last_pull_time.isoformat())\n",
    "            # HERE TEST THIS NEW DATA FOR ANOMALY DETECTION.\n",
    "            if new_df.empty:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            \n",
    "            anomaly_flagged_df = self.detect_anomalies(new_df)\n",
    "            only_anomaly = (anomaly_flagged_df[anomaly_flagged_df.anomaly])\n",
    "            if only_anomaly.empty:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            print(only_anomaly)\n",
    "            # ADD NON-ANOMALY DATA TO BUFFER FOR TRAINIG\n",
    "            self.extend_non_anomaly_df(anomaly_flagged_df)\n",
    "            \n",
    "            # IF BUFFER HAS ONE YEAR, TRAIN NEW DATA   \n",
    "            if self.has_one_year(self.upcoming_df):\n",
    "                self.training_df = self.upcoming_df\n",
    "                self.upcoming_df = pd.DataFrame()\n",
    "            \n",
    "            break\n",
    "            time.sleep(5)\n",
    "            \n",
    "    def has_one_year(self, df):\n",
    "        time_span = df['original_time'].max() - df['original_time'].min()\n",
    "        return time_span >= pd.Timedelta(days=365)\n",
    "    \n",
    "    def extend_non_anomaly_df(self,flagged_df):\n",
    "        #get non_anomaly data\n",
    "        non_anomaly_data= flagged_df[~flagged_df.anomaly][['noisy','original_time']]\n",
    "        \n",
    "        # extend future training data\n",
    "        self.upcoming_df = pd.concat([self.upcoming_df,non_anomaly_data])\n",
    "    \n",
    "    def train_model(self):\n",
    "        pass\n",
    "    \n",
    "    def detect_anomalies(self, new_df):\n",
    "        # Simple Anomaly Detection (Just for testing the class)\n",
    "        new_df['threshold_anomaly'] = new_df['noisy'] > 70\n",
    "        \n",
    "        \n",
    "        # Final decision for classification\n",
    "        new_df['anomaly'] = new_df['threshold_anomaly'] \n",
    "        return new_df\n",
    "    \n",
    "    def get_dataframe_starting(self, starting_time):\n",
    "        query = f'''\n",
    "        from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "                |> range(start: {starting_time})\n",
    "                |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
    "                |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
    "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                |> keep(columns: [\"noisy\", \"original_time\"])\n",
    "        '''\n",
    "        df = self.query_api.query_data_frame(query=query)\n",
    "        client.close()\n",
    "        if not df.empty:\n",
    "            df.drop(columns=['table','result'],inplace=True)\n",
    "            df['original_time'] = pd.to_datetime(df['original_time'])\n",
    "            self.last_pull_time = datetime.now().replace(tzinfo=timezone.utc)\n",
    "        return  df\n",
    "    \n",
    "    def get_dataframe_initial(self):\n",
    "        checkpoint_df = self.get_dataframe_starting('-5h')\n",
    "        latest_time = checkpoint_df['original_time'].max()\n",
    "        one_year_before_latest = latest_time - pd.DateOffset(years=1)\n",
    "        training_df = checkpoint_df[checkpoint_df['original_time'] >= one_year_before_latest]\n",
    "\n",
    "        self.training_df = training_df\n",
    "\n",
    "\n",
    "client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)\n",
    "ad = AnomalyDetector(client)\n",
    "ad.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noisy</th>\n",
       "      <th>original_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12033</th>\n",
       "      <td>70.090403</td>\n",
       "      <td>2025-05-07 18:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>70.301699</td>\n",
       "      <td>2025-05-08 09:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12035</th>\n",
       "      <td>69.996953</td>\n",
       "      <td>2025-05-08 12:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12036</th>\n",
       "      <td>71.090618</td>\n",
       "      <td>2025-05-08 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12037</th>\n",
       "      <td>70.555684</td>\n",
       "      <td>2025-05-08 18:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           noisy             original_time\n",
       "12033  70.090403 2025-05-07 18:00:00+00:00\n",
       "12034  70.301699 2025-05-08 09:00:00+00:00\n",
       "12035  69.996953 2025-05-08 12:00:00+00:00\n",
       "12036  71.090618 2025-05-08 15:00:00+00:00\n",
       "12037  70.555684 2025-05-08 18:00:00+00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "query = f'''\n",
    "from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "        |> range(start: -5h)\n",
    "        |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
    "        |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        |> keep(columns: [\"noisy\", \"original_time\"])\n",
    "'''\n",
    "checkpoint_df = query_api.query_data_frame(query=query)\n",
    "client.close()\n",
    "checkpoint_df.drop(columns=['table','result'],inplace=True)\n",
    "checkpoint_df['original_time'] = pd.to_datetime(checkpoint_df['original_time'])\n",
    "latest_time = checkpoint_df['original_time'].max()\n",
    "one_year_before_latest = latest_time - pd.DateOffset(years=1)\n",
    "training_df = checkpoint_df[checkpoint_df['original_time'] >= one_year_before_latest]\n",
    "\n",
    "training_df.head()\n",
    "\n",
    "# HERE TRAIN DATA USING TRAINING DF\n",
    "# trainanomalymodel(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-15T10:02:05.307399+00:00\n",
      "        noisy             original_time\n",
      "0   63.564632 2022-06-13 18:00:00+00:00\n",
      "1   63.018735 2022-06-14 09:00:00+00:00\n",
      "2   63.673516 2022-06-14 12:00:00+00:00\n",
      "3   63.469121 2022-06-14 15:00:00+00:00\n",
      "4   63.836436 2022-06-14 18:00:00+00:00\n",
      "5   61.043874 2022-06-15 09:00:00+00:00\n",
      "6   60.598961 2022-06-15 12:00:00+00:00\n",
      "7   60.869564 2022-06-15 15:00:00+00:00\n",
      "8   60.705201 2022-06-15 18:00:00+00:00\n",
      "9   61.193341 2022-06-16 09:00:00+00:00\n",
      "10  61.037952 2022-06-16 12:00:00+00:00\n",
      "11  61.067039 2022-06-16 15:00:00+00:00\n",
      "12  60.969226 2022-06-16 18:00:00+00:00\n",
      "13  57.462581 2022-06-17 09:00:00+00:00\n",
      "14  57.635690 2022-06-17 12:00:00+00:00\n",
      "15  57.939654 2022-06-17 15:00:00+00:00\n",
      "16  57.995920 2022-06-17 18:00:00+00:00\n",
      "17  59.530087 2022-06-18 09:00:00+00:00\n",
      "18  59.437133 2022-06-18 12:00:00+00:00\n",
      "19  59.359975 2022-06-18 15:00:00+00:00\n",
      "20  60.244319 2022-06-18 18:00:00+00:00\n",
      "21  57.581128 2022-06-19 09:00:00+00:00\n",
      "22  58.253188 2022-06-19 12:00:00+00:00\n",
      "23  58.341501 2022-06-19 15:00:00+00:00\n",
      "24  58.046356 2022-06-19 18:00:00+00:00\n",
      "25  57.384845 2022-06-20 09:00:00+00:00\n",
      "26  57.511496 2022-06-20 12:00:00+00:00\n",
      "27  57.704859 2022-06-20 15:00:00+00:00\n",
      "28  58.241423 2022-06-20 18:00:00+00:00\n",
      "29  61.542244 2022-06-21 09:00:00+00:00\n",
      "30  60.781354 2022-06-21 12:00:00+00:00\n",
      "31  61.675816 2022-06-21 15:00:00+00:00\n",
      "32  61.255087 2022-06-21 18:00:00+00:00\n",
      "33  60.087798 2022-06-22 09:00:00+00:00\n",
      "34  60.239361 2022-06-22 12:00:00+00:00\n",
      "35  60.191410 2022-06-22 15:00:00+00:00\n",
      "36  60.167187 2022-06-22 18:00:00+00:00\n",
      "37  61.196494 2022-06-23 09:00:00+00:00\n",
      "38  61.112748 2022-06-23 12:00:00+00:00\n",
      "39  61.122631 2022-06-23 15:00:00+00:00\n",
      "40  61.066880 2022-06-23 18:00:00+00:00\n",
      "41  60.792225 2022-06-24 09:00:00+00:00\n",
      "42  60.918366 2022-06-24 12:00:00+00:00\n",
      "43  60.794942 2022-06-24 15:00:00+00:00\n",
      "44  60.939983 2022-06-24 18:00:00+00:00\n",
      "45  61.149171 2022-06-25 09:00:00+00:00\n",
      "46  61.808403 2022-06-25 12:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timezone\n",
    "import time\n",
    "\n",
    "last_pull_time = datetime.utcnow().replace(tzinfo=timezone.utc) - pd.DateOffset(second=5)\n",
    "\n",
    "print(last_pull_time.isoformat())\n",
    "while True:\n",
    "    # Build the query with dynamic start time\n",
    "    query = f'''\n",
    "    from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "        |> range(start: {last_pull_time.isoformat()})\n",
    "        |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
    "        |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        |> keep(columns: [\"noisy\", \"original_time\"])\n",
    "    '''\n",
    "    new_data = query_api.query_data_frame(query=query)\n",
    "    \n",
    "    if not new_data.empty:\n",
    "        new_data.drop(columns=['table', 'result'], inplace=True)\n",
    "        new_data['original_time'] = pd.to_datetime(new_data['original_time'])\n",
    "        \n",
    "        # HERE TEST THIS NEW DATA FOR ANOMALY DETECTION.\n",
    "        # detect_anomalies\n",
    "        # ADD NON_ANOMALY ROWS TO ANOTHER DATAFRAME\n",
    "        # \n",
    "        # IF ANOTHERDATAFRAME HAS 1 YEAR OF DATA, TRAIN DATA ON THAT.        \n",
    "        #checkpoint_df = pd.concat([checkpoint_df, new_data])\n",
    "        print(new_data)\n",
    "        # Update the last pull time\n",
    "        #last_pull_time = latest_time\n",
    "        last_pull_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Wait for 5 seconds before the next pull\n",
    "    break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6573 entries, 0 to 6572\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   noisy          6564 non-null   float64            \n",
      " 1   original_time  6573 non-null   datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), float64(1)\n",
      "memory usage: 102.8 KB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = result['original_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-08-09 12:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
