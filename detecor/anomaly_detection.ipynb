{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "# def read_secret(secret_name):\n",
    "#     secret_path = os.getenv(secret_name)\n",
    "#     try:\n",
    "#         with open(secret_path, 'r') as file:\n",
    "#             return file.read().strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {secret_name}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# INFLUXDB_USERNAME = read_secret('DOCKER_INFLUXDB_INIT_USERNAME_FILE')\n",
    "# INFLUXDB_PASSWORD = read_secret('DOCKER_INFLUXDB_INIT_PASSWORD_FILE')\n",
    "# INFLUXDB_TOKEN = read_secret('DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE')\n",
    "\n",
    "# INFLUXDB_ORG = os.getenv('DOCKER_INFLUXDB_INIT_ORG')\n",
    "# INFLUXDB_BUCKET = os.getenv('DOCKER_INFLUXDB_INIT_BUCKET')\n",
    "# INFLUXDB_URL = os.getenv('INFLUXDB_URL', 'http://influxdb:8086') \n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "from influxdb_client.client.query_api import QueryOptions\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "INFLUXDB_USERNAME=\"admin\"\n",
    "INFLUXDB_PASSWORD=\"password\"\n",
    "INFLUXDB_TOKEN=\"G3UtSut5Kv-RuT32yh27StdDCrl4fu3uzxPLzdias8vFsNzyzgfw5kIX9iGvtLctAXpZjFItOUwA65YWtk_5fg==\"\n",
    "INFLUXDB_ORG=\"example_org\"\n",
    "INFLUXDB_BUCKET=\"example_bucket\"\n",
    "INFLUXDB_URL=\"http://localhost:8086\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "class AnomalyDetector():\n",
    "    def __init__(self,url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG,bucket=INFLUXDB_BUCKET):\n",
    "        self.url = url\n",
    "        self.token=token\n",
    "        self.org = org\n",
    "        self.bucket=bucket\n",
    "        self.client = InfluxDBClient(url=self.url, token=self.token, org=self.org)\n",
    "        self.query_api = self.client.query_api(query_options=QueryOptions(profilers=[]))\n",
    "        \n",
    "        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)\n",
    "        self.training_df = pd.DataFrame()\n",
    "        self.upcoming_df = pd.DataFrame()\n",
    "        self.bucket=bucket\n",
    "        self.last_pull_time = datetime.now()\n",
    "    def run(self):\n",
    "        self.get_dataframe_initial()\n",
    "        self.train_model()\n",
    "        while True:\n",
    "            new_df = self.get_dataframe_starting(self.last_pull_time)\n",
    "            # HERE TEST THIS NEW DATA FOR ANOMALY DETECTION.\n",
    "            if new_df.empty:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            \n",
    "            # DETECT ANOMALIES\n",
    "            anomaly_flagged_df = self.detect_anomalies(new_df)\n",
    "            self.push_data_to_influxdb(anomaly_flagged_df)\n",
    "            \n",
    "            # # FOR DEBUGGING ONLY\n",
    "            # only_anomaly = (anomaly_flagged_df[anomaly_flagged_df.anomaly])\n",
    "            # if only_anomaly.empty:\n",
    "            #     time.sleep(5)\n",
    "            #     continue\n",
    "            # print(only_anomaly)\n",
    "            \n",
    "            # ADD NON-ANOMALY DATA TO BUFFER FOR TRAINIG\n",
    "            self.extend_non_anomaly_df(anomaly_flagged_df)\n",
    "            \n",
    "            # IF BUFFER HAS ONE YEAR, TRAIN NEW DATA   \n",
    "            if self.has_one_year(self.upcoming_df):\n",
    "                self.training_df = self.upcoming_df\n",
    "                self.upcoming_df = pd.DataFrame()\n",
    "            \n",
    "            \n",
    "            time.sleep(5)\n",
    "    def push_data_to_influxdb(self,df):\n",
    "\n",
    "        points = [\n",
    "            Point(\"anomaly_data\")\n",
    "            .tag(\"unit\", \"bpm\")\n",
    "            .field(\"noisy\", row['noisy'])\n",
    "            .field(\"anomaly\", row['anomaly'])\n",
    "            .field(\"original_time\", row['original_time'].isoformat())  # Storing original time as a field\n",
    "            .time(datetime.utcnow(),WritePrecision.MS)  # Use current UTC time for timestamp\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        self.write_api.write(bucket=self.bucket, record=points)\n",
    "            \n",
    "    def has_one_year(self, df):\n",
    "        time_span = df['original_time'].max() - df['original_time'].min()\n",
    "        return time_span >= pd.Timedelta(days=365)\n",
    "    \n",
    "    def extend_non_anomaly_df(self,flagged_df):\n",
    "        #get non_anomaly data\n",
    "        non_anomaly_data= flagged_df[~flagged_df.anomaly][['noisy','original_time']]\n",
    "        \n",
    "        # extend future training data\n",
    "        self.upcoming_df = pd.concat([self.upcoming_df,non_anomaly_data])\n",
    "    \n",
    "    def train_model(self):\n",
    "        pass\n",
    "    \n",
    "    def detect_anomalies(self, new_df):\n",
    "        # Simple Anomaly Detection (Just for testing the class)\n",
    "        new_df['threshold_anomaly'] = new_df['noisy'] > 70\n",
    "        \n",
    "        \n",
    "        # Final decision for classification\n",
    "        new_df['anomaly'] = new_df['threshold_anomaly'] \n",
    "        return new_df\n",
    "    \n",
    "    def get_dataframe_starting(self, starting_time):\n",
    "\n",
    "        query = f'''\n",
    "        from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "                |> range(start: {starting_time})\n",
    "                |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
    "                |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
    "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                |> keep(columns: [\"noisy\", \"original_time\"])\n",
    "        '''\n",
    "        print(query)\n",
    "        df = self.query_api.query_data_frame(query=query)\n",
    "        if not df.empty:\n",
    "            self.last_pull_time = (datetime.utcnow().replace(tzinfo=pytz.utc) -timedelta(seconds=10)).isoformat()\n",
    "            df.drop(columns=['table','result'],inplace=True)\n",
    "            df['original_time'] = pd.to_datetime(df['original_time'])\n",
    "            if not self.upcoming_df.empty:\n",
    "                mask = ~df['original_time'].isin(self.upcoming_df['original_time'])\n",
    "                df = df[mask]\n",
    "        return  df\n",
    "    \n",
    "    def get_dataframe_initial(self):\n",
    "        checkpoint_df = self.get_dataframe_starting('-24h')\n",
    "        latest_time = checkpoint_df['original_time'].max()\n",
    "        one_year_before_latest = latest_time - pd.DateOffset(years=1)\n",
    "        training_df = checkpoint_df[checkpoint_df['original_time'] >= one_year_before_latest]\n",
    "        training_df['anomaly'] = False\n",
    "        self.push_data_to_influxdb(training_df)\n",
    "        self.training_df = training_df\n",
    "\n",
    "\n",
    "ad = AnomalyDetector(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG,bucket=INFLUXDB_BUCKET)\n",
    "#ad.run()\n",
    "#ad.client.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-09-15T16:05:42.776830'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.last_pull_time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        from(bucket: \"example_bucket\")\n",
      "                |> range(start: -24h)\n",
      "                |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
      "                |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
      "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
      "                |> keep(columns: [\"noisy\", \"original_time\"])\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4150407/2606115452.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df['anomaly'] = False\n"
     ]
    }
   ],
   "source": [
    "ad.get_dataframe_initial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noisy</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>rolling_mean</th>\n",
       "      <th>rolling_std</th>\n",
       "      <th>rolling_Q1</th>\n",
       "      <th>rolling_Q3</th>\n",
       "      <th>rolling_IQR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-26 11:59:32+00:00</th>\n",
       "      <td>65.723971</td>\n",
       "      <td>False</td>\n",
       "      <td>65.631838</td>\n",
       "      <td>0.130295</td>\n",
       "      <td>65.585772</td>\n",
       "      <td>65.677905</td>\n",
       "      <td>0.092133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-27 11:59:32+00:00</th>\n",
       "      <td>65.890695</td>\n",
       "      <td>False</td>\n",
       "      <td>65.718124</td>\n",
       "      <td>0.175568</td>\n",
       "      <td>65.631838</td>\n",
       "      <td>65.807333</td>\n",
       "      <td>0.175495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-28 11:59:32+00:00</th>\n",
       "      <td>65.035382</td>\n",
       "      <td>False</td>\n",
       "      <td>65.547439</td>\n",
       "      <td>0.370248</td>\n",
       "      <td>65.413625</td>\n",
       "      <td>65.765652</td>\n",
       "      <td>0.352027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-29 11:59:32+00:00</th>\n",
       "      <td>66.481153</td>\n",
       "      <td>False</td>\n",
       "      <td>65.734181</td>\n",
       "      <td>0.526476</td>\n",
       "      <td>65.539705</td>\n",
       "      <td>65.890695</td>\n",
       "      <td>0.350990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30 11:59:32+00:00</th>\n",
       "      <td>70.460331</td>\n",
       "      <td>False</td>\n",
       "      <td>66.521873</td>\n",
       "      <td>1.986074</td>\n",
       "      <td>65.585772</td>\n",
       "      <td>66.333539</td>\n",
       "      <td>0.747767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-23 18:00:00+00:00</th>\n",
       "      <td>66.889781</td>\n",
       "      <td>False</td>\n",
       "      <td>73.729311</td>\n",
       "      <td>26.329177</td>\n",
       "      <td>66.565439</td>\n",
       "      <td>67.189153</td>\n",
       "      <td>0.623714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-24 09:00:00+00:00</th>\n",
       "      <td>67.711627</td>\n",
       "      <td>False</td>\n",
       "      <td>73.733448</td>\n",
       "      <td>26.328186</td>\n",
       "      <td>66.565439</td>\n",
       "      <td>67.189153</td>\n",
       "      <td>0.623714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-24 12:00:00+00:00</th>\n",
       "      <td>66.619458</td>\n",
       "      <td>False</td>\n",
       "      <td>73.720807</td>\n",
       "      <td>26.331637</td>\n",
       "      <td>66.565439</td>\n",
       "      <td>67.189153</td>\n",
       "      <td>0.623714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-24 15:00:00+00:00</th>\n",
       "      <td>66.817401</td>\n",
       "      <td>False</td>\n",
       "      <td>73.720763</td>\n",
       "      <td>26.331649</td>\n",
       "      <td>66.565439</td>\n",
       "      <td>67.189153</td>\n",
       "      <td>0.623714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-24 18:00:00+00:00</th>\n",
       "      <td>66.942663</td>\n",
       "      <td>False</td>\n",
       "      <td>73.710970</td>\n",
       "      <td>26.334208</td>\n",
       "      <td>66.565439</td>\n",
       "      <td>67.149365</td>\n",
       "      <td>0.583925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               noisy  anomaly  rolling_mean  rolling_std  \\\n",
       "original_time                                                              \n",
       "2016-05-26 11:59:32+00:00  65.723971    False     65.631838     0.130295   \n",
       "2016-05-27 11:59:32+00:00  65.890695    False     65.718124     0.175568   \n",
       "2016-05-28 11:59:32+00:00  65.035382    False     65.547439     0.370248   \n",
       "2016-05-29 11:59:32+00:00  66.481153    False     65.734181     0.526476   \n",
       "2016-05-30 11:59:32+00:00  70.460331    False     66.521873     1.986074   \n",
       "...                              ...      ...           ...          ...   \n",
       "2017-05-23 18:00:00+00:00  66.889781    False     73.729311    26.329177   \n",
       "2017-05-24 09:00:00+00:00  67.711627    False     73.733448    26.328186   \n",
       "2017-05-24 12:00:00+00:00  66.619458    False     73.720807    26.331637   \n",
       "2017-05-24 15:00:00+00:00  66.817401    False     73.720763    26.331649   \n",
       "2017-05-24 18:00:00+00:00  66.942663    False     73.710970    26.334208   \n",
       "\n",
       "                           rolling_Q1  rolling_Q3  rolling_IQR  \n",
       "original_time                                                   \n",
       "2016-05-26 11:59:32+00:00   65.585772   65.677905     0.092133  \n",
       "2016-05-27 11:59:32+00:00   65.631838   65.807333     0.175495  \n",
       "2016-05-28 11:59:32+00:00   65.413625   65.765652     0.352027  \n",
       "2016-05-29 11:59:32+00:00   65.539705   65.890695     0.350990  \n",
       "2016-05-30 11:59:32+00:00   65.585772   66.333539     0.747767  \n",
       "...                               ...         ...          ...  \n",
       "2017-05-23 18:00:00+00:00   66.565439   67.189153     0.623714  \n",
       "2017-05-24 09:00:00+00:00   66.565439   67.189153     0.623714  \n",
       "2017-05-24 12:00:00+00:00   66.565439   67.189153     0.623714  \n",
       "2017-05-24 15:00:00+00:00   66.565439   67.189153     0.623714  \n",
       "2017-05-24 18:00:00+00:00   66.565439   67.149365     0.583925  \n",
       "\n",
       "[409 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf= ad.training_df.set_index('original_time').copy()\n",
    "# Define the window size for the rolling calculations\n",
    "window_size = '7D'  # Example for daily rolling window if data is hourly\n",
    "\n",
    "# Calculate rolling mean and standard deviation\n",
    "tdf['rolling_mean'] = tdf['noisy'].rolling(window=window_size).mean()\n",
    "tdf['rolling_std'] = tdf['noisy'].rolling(window=window_size).std()\n",
    "tdf['rolling_Q1'] = tdf['noisy'].rolling(window=window_size).quantile(0.25)\n",
    "tdf['rolling_Q3'] = tdf['noisy'].rolling(window=window_size).quantile(0.75)\n",
    "tdf['rolling_IQR'] = tdf['rolling_Q3'] - tdf['rolling_Q1']\n",
    "tdf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the year for seasonal overlap\n",
    "\n",
    "tdf.index = pd.to_datetime(tdf.index)\n",
    "def convert_to_month_day(df):\n",
    "    # Format the index to 'month-day', ensuring the index is datetime\n",
    "    month_day_index = df.index.strftime('%m-%d')  # Convert datetime index to 'month-day' string\n",
    "    df.index = month_day_index  # Set this new index\n",
    "    return df\n",
    "tdf = convert_to_month_day(tdf.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        from(bucket: \"example_bucket\")\n",
      "                |> range(start: -20s)\n",
      "                |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
      "                |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
      "                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
      "                |> keep(columns: [\"noisy\", \"original_time\"])\n",
      "        \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoisy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7D\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     20\u001b[0m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoisy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7D\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m---> 21\u001b[0m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_std_old\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrolling_std\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_mean_old\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreindex(new_df\u001b[38;5;241m.\u001b[39mindex, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m std_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n",
      "File \u001b[0;32m~/tools/cobblestone/.venv/lib/python3.8/site-packages/pandas/core/series.py:4918\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   4901\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   4902\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   4903\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4916\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 4918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/cobblestone/.venv/lib/python3.8/site-packages/pandas/core/generic.py:5360\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5359\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m   5362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tools/cobblestone/.venv/lib/python3.8/site-packages/pandas/core/generic.py:5375\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5372\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5374\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5375\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[1;32m   5377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5379\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5380\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5381\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5382\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5383\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5384\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5385\u001b[0m )\n",
      "File \u001b[0;32m~/tools/cobblestone/.venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:4275\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   4274\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[0;32m-> 4275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4277\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "new_df = ad.get_dataframe_starting(\"-20s\")\n",
    "\n",
    "new_df.index = pd.to_datetime(new_df.index)\n",
    "\n",
    "\n",
    "# Function to convert index to 'month-day' format\n",
    "\n",
    "# Apply this function to both dataframes\n",
    "new_df = convert_to_month_day(new_df.copy())\n",
    "\n",
    "new_df['original_time'] = pd.to_datetime(new_df['original_time'])\n",
    "new_df.set_index('original_time', inplace=True)\n",
    "std_multiplier = 3  # Define how many standard deviations away to flag as anomaly\n",
    "\n",
    "new_df['rolling_mean'] = new_df['noisy'].rolling(window='7D').mean()\n",
    "new_df['rolling_std'] = new_df['noisy'].rolling(window='7D').std()\n",
    "new_df['rolling_std_old'] = tdf['rolling_std'].reindex(new_df.index, method='nearest')\n",
    "new_df['rolling_mean_old'] = tdf['rolling_mean'].reindex(new_df.index, method='nearest')\n",
    "\n",
    "std_threshold = 2 \n",
    "new_df['exceeds_threshold'] = new_df['rolling_std'] > std_threshold *new_df['rolling_std_old']\n",
    "\n",
    "def identify_anomalies(row):\n",
    "    if row['exceeds_threshold']:\n",
    "        # Flag as anomaly if the deviation from the mean is unusually large\n",
    "        return abs(row['noisy'] - row['rolling_mean']) > 1.7 * row['rolling_std']\n",
    "    return False\n",
    "\n",
    "# Apply the function\n",
    "new_df['anomaly'] = new_df.apply(identify_anomalies, axis=1)\n",
    "cutoff_date = new_df.index.min() + pd.Timedelta(days=7)\n",
    "new_df = new_df[new_df.index > cutoff_date]\n",
    "plot_df = new_df.copy()\n",
    "plot_tdf = tdf.copy()\n",
    "plot_df.index = plot_df.index.map(lambda x: x.replace(year=2000))\n",
    "plot_tdf.index = plot_tdf.index.map(lambda x: x.replace(year=2000))\n",
    "anomalies = plot_df[plot_df['anomaly']]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the figure with three subplots horizontally\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# First subplot: Plot all noisy data and highlight anomalies\n",
    "ax1.plot(new_df.index, new_df['noisy'], label='Noisy Data', color='blue', linestyle='--', marker='o', markersize=5, alpha=0.7)\n",
    "anomalies = new_df[new_df['anomaly'] == True]\n",
    "ax1.scatter(anomalies.index, anomalies['noisy'], color='red', label='Anomalies', marker='o', s=50)\n",
    "ax1.set_title('Noisy Data with Anomalies Highlighted')\n",
    "ax1.set_xlabel('Date (MM-DD)')\n",
    "ax1.set_ylabel('Noisy Values')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Second subplot: Historical mean vs. Current mean\n",
    "ax2.plot(new_df.index, new_df['rolling_mean'], label='Current Rolling Mean', color='green', linestyle='-', linewidth=2)\n",
    "ax2.plot(tdf.index, tdf['rolling_mean'], label='Historical Rolling Mean', color='purple', linestyle=':', linewidth=2)\n",
    "ax2.set_title('Rolling Means Comparison')\n",
    "ax2.set_xlabel('Date (MM-DD)')\n",
    "ax2.set_ylabel('Mean Values')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Third subplot: Historical std vs. Current std\n",
    "ax3.plot(new_df.index, new_df['rolling_std'], label='Current Rolling Std', color='orange', linestyle='-', linewidth=2)\n",
    "ax3.plot(tdf.index, tdf['rolling_std'], label='Historical Rolling Std', color='cyan', linestyle=':', linewidth=2)\n",
    "ax3.set_title('Rolling Standard Deviations Comparison')\n",
    "ax3.set_xlabel('Date (MM-DD)')\n",
    "ax3.set_ylabel('Standard Deviation')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# Improve formatting of the x-axis dates\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if not anomalies.empty:\n",
    "    print(\"Detailed Anomaly Information:\")\n",
    "    for idx, row in anomalies.iterrows():\n",
    "        print(f\"Time: {idx}\")\n",
    "        print(f\"  Anomaly Noisy Value: {row['noisy']}\")\n",
    "        print(f\"  Rolling Mean at Time of Anomaly: {row['rolling_mean']}\")\n",
    "        print(f\"  Rolling Standard Deviation at Time of Anomaly: {row['rolling_std']}\")\n",
    "        print(f\"  Old Rolling Standard Deviation from Training Data: {row[ 'rolling_std_old']}\")\n",
    "        print(\"------\")\n",
    "else:\n",
    "    print(\"No anomalies detected within the specified parameters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noisy</th>\n",
       "      <th>original_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12033</th>\n",
       "      <td>70.090403</td>\n",
       "      <td>2025-05-07 18:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>70.301699</td>\n",
       "      <td>2025-05-08 09:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12035</th>\n",
       "      <td>69.996953</td>\n",
       "      <td>2025-05-08 12:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12036</th>\n",
       "      <td>71.090618</td>\n",
       "      <td>2025-05-08 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12037</th>\n",
       "      <td>70.555684</td>\n",
       "      <td>2025-05-08 18:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           noisy             original_time\n",
       "12033  70.090403 2025-05-07 18:00:00+00:00\n",
       "12034  70.301699 2025-05-08 09:00:00+00:00\n",
       "12035  69.996953 2025-05-08 12:00:00+00:00\n",
       "12036  71.090618 2025-05-08 15:00:00+00:00\n",
       "12037  70.555684 2025-05-08 18:00:00+00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "query = f'''\n",
    "from(bucket: \"{INFLUXDB_BUCKET}\")\n",
    "        |> range(start: -5h)\n",
    "        |> filter(fn: (r) => r._measurement == \"heart_rate\")\n",
    "        |> filter(fn: (r) => r._field == \"noisy\" or r._field == \"original_time\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        |> keep(columns: [\"noisy\", \"original_time\"])\n",
    "'''\n",
    "checkpoint_df = query_api.query_data_frame(query=query)\n",
    "client.close()\n",
    "checkpoint_df.drop(columns=['table','result'],inplace=True)\n",
    "checkpoint_df['original_time'] = pd.to_datetime(checkpoint_df['original_time'])\n",
    "latest_time = checkpoint_df['original_time'].max()\n",
    "one_year_before_latest = latest_time - pd.DateOffset(years=1)\n",
    "training_df = checkpoint_df[checkpoint_df['original_time'] >= one_year_before_latest]\n",
    "\n",
    "training_df.head()\n",
    "\n",
    "# HERE TRAIN DATA USING TRAINING DF\n",
    "# trainanomalymodel(training_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
